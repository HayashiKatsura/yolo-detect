import os
pj_folder = os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
import sys
sys.path.append(pj_folder)    
from _project.mydata._code.yolo._train import standard_train
from flask import Flask, request, jsonify
from flask_cors import CORS
import threading
import json
import pandas as pd
import os
import time
import uuid
import yaml
import gc
import torch
import signal
import subprocess
import psutil
import zipfile
from pathlib import Path
from watchdog.observers import Observer
from watchdog.events import FileSystemEventHandler
import logging
import glob
project_path = os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
import sys
sys.path.append(project_path)
from uuid import uuid4
from _api._utils.DataRecord import data_record
from _api._utils.IsFilesExist import FilesID,is_files_exist,files_info
from _api.configuration.FilesRecordMapping import files_record_mapping,FilesType,table_1, table_2,table_3,table_4,table_5
from _api.entity.SQLModels import db, FileTable, DetectionTable, DatasetTable, WeightTable, create_db
from _api.configuration.handle_db_error import handle_db_error

# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)
# from loguru import logger as lgu
# lgu.remove()
# app = Flask(__name__)
# CORS(app)


def get_training_manager():
    return YOLOTrainingManager()

class CSVMonitorHandler(FileSystemEventHandler):
    def __init__(self, session_id, csv_path, manager):
        self.session_id = session_id
        self.csv_path = csv_path
        self.manager = manager
        self.last_row_count = 0
        self.last_modified = 0
        self.last_data_hash = None
        self.time_stamp =str(time.strftime("%Y%m%d%H%M", time.localtime()))
        logger.info(f"ğŸ“ å¼€å§‹ç›‘æ§CSVæ–‡ä»¶: {csv_path}")
    
    def on_modified(self, event):
        if not event.is_directory and event.src_path == self.csv_path:
            current_time = time.time()
            if current_time - self.last_modified < 0.5:
                return
            self.last_modified = current_time
            
            logger.info(f"æ£€æµ‹åˆ°CSVæ–‡ä»¶å˜åŒ–: {event.src_path}")
            self.read_and_update()
    
    def read_and_update(self):
        """è¯»å–CSVå¹¶æ›´æ–°æ•°æ®"""
        try:
            if not os.path.exists(self.csv_path):
                logger.warning(f"CSVæ–‡ä»¶ä¸å­˜åœ¨: {self.csv_path}")
                return
                
            df = None
            for attempt in range(5):
                try:
                    df = pd.read_csv(self.csv_path)
                    break
                except (pd.errors.EmptyDataError, FileNotFoundError, pd.errors.ParserError) as e:
                    if attempt < 4:
                        time.sleep(0.2)
                        continue
                    else:
                        logger.warning(f"CSVæ–‡ä»¶è¯»å–å¤±è´¥ï¼Œå°è¯•{attempt+1}æ¬¡: {e}")
                        return
                except Exception as e:
                    logger.error(f"CSVè¯»å–å¼‚å¸¸: {e}")
                    return
            
            if df is None or len(df) == 0:
                logger.warning("CSVæ–‡ä»¶ä¸ºç©º")
                return
                
            current_row_count = len(df)
            latest_row = df.iloc[-1]
            
            data_hash = hash(str(latest_row.to_dict()))
            if data_hash == self.last_data_hash and current_row_count == self.last_row_count:
                return
            
            if current_row_count == 1:
                logger.info(f"CSVåˆ—å: {list(df.columns)}")
            
            # è·å–ç”¨æˆ·é…ç½®çš„æ€»è½®æ¬¡
            session_info = self.manager.active_processes.get(self.session_id, {})
            configured_epochs = session_info.get('params', {}).get('epochs', 100)
            
            progress_data = self._parse_yolo_data(latest_row, current_row_count, df.columns, configured_epochs)
            
            if progress_data:
                if self.session_id in self.manager.active_processes:
                    self.manager.active_processes[self.session_id]['latest_data'] = progress_data
                    logger.info(f"ğŸ“ˆ æ›´æ–°è¿›åº¦: Epoch {progress_data['epoch']}, mAP50: {progress_data['metrics']['mAP50']:.3f}")
                
                self.last_row_count = current_row_count
                self.last_data_hash = data_hash
                
        except Exception as e:
            logger.error(f"âŒ CSVè¯»å–é”™è¯¯: {e}")
    
    def _parse_yolo_data(self, row, total_rows, columns, configured_epochs):
        """è§£æYOLOæ•°æ® - ä¿®å¤è½®æ¬¡æ˜¾ç¤º"""
        try:
            logger.debug(f"å¯ç”¨åˆ—: {list(columns)}")
            
            epoch = 0
            for col in ['epoch', 'Epoch']:
                if col in row:
                    epoch = int(float(row[col])) + 1
                    break
            
            if epoch == 0:
                logger.warning("æœªæ‰¾åˆ°epochåˆ—")
                return None
            
            train_box_loss = self._get_value_by_possible_names(row, [
                'train/box_loss', 'box_loss', 'train_box_loss', 'Box'
            ])
            train_obj_loss = self._get_value_by_possible_names(row, [
                'train/obj_loss', 'obj_loss', 'train_obj_loss', 'Objectness', 'train/cls_loss'
            ])
            train_cls_loss = self._get_value_by_possible_names(row, [
                'train/cls_loss', 'cls_loss', 'train_cls_loss', 'Classification'
            ])
            
            val_box_loss = self._get_value_by_possible_names(row, [
                'val/box_loss', 'val_box_loss', 'val/Box'
            ])
            val_obj_loss = self._get_value_by_possible_names(row, [
                'val/obj_loss', 'val_obj_loss', 'val/Objectness', 'val/cls_loss'
            ])
            val_cls_loss = self._get_value_by_possible_names(row, [
                'val/cls_loss', 'val_cls_loss', 'val/Classification'
            ])
            
            precision = self._get_value_by_possible_names(row, [
                'metrics/precision(B)', 'metrics/precision', 'precision', 'Precision', 'P'
            ])
            recall = self._get_value_by_possible_names(row, [
                'metrics/recall(B)', 'metrics/recall', 'recall', 'Recall', 'R'
            ])
            map50 = self._get_value_by_possible_names(row, [
                'metrics/mAP50(B)', 'metrics/mAP_0.5', 'metrics/mAP@0.5', 'mAP50', 'mAP_0.5', 'mAP@0.5'
            ])
            map50_95 = self._get_value_by_possible_names(row, [
                'metrics/mAP50-95(B)', 'metrics/mAP_0.5:0.95', 'metrics/mAP@0.5:0.95', 'mAP50-95', 'mAP_0.5:0.95'
            ])
            
            lr = self._get_value_by_possible_names(row, [
                'lr/pg0', 'lr/pg1', 'lr/pg2', 'learning_rate', 'lr', 'LR'
            ])
            
            result = {
                "epoch": epoch,
                "total_epochs": configured_epochs,  # ä½¿ç”¨ç”¨æˆ·é…ç½®çš„è½®æ¬¡
                "train_losses": {
                    "box_loss": train_box_loss,
                    "obj_loss": train_obj_loss,
                    "cls_loss": train_cls_loss,
                    "total_loss": train_box_loss + train_obj_loss + train_cls_loss
                },
                "val_losses": {
                    "box_loss": val_box_loss,
                    "obj_loss": val_obj_loss,
                    "cls_loss": val_cls_loss,
                    "total_loss": val_box_loss + val_obj_loss + val_cls_loss
                },
                "metrics": {
                    "precision": precision,
                    "recall": recall,
                    "mAP50": map50,
                    "mAP50_95": map50_95
                },
                "learning_rate": lr,
                "timestamp": time.time()
            }
            
            logger.debug(f"è§£æç»“æœ: Epoch {epoch}/{configured_epochs}, Box: {train_box_loss:.4f}, mAP50: {map50:.4f}")
            return result
            
        except Exception as e:
            logger.error(f"âŒ æ•°æ®è§£æé”™è¯¯: {e}")
            return None
    
    def _get_value_by_possible_names(self, row, possible_names):
        """æ ¹æ®å¯èƒ½çš„åˆ—åè·å–å€¼"""
        for name in possible_names:
            if name in row and pd.notna(row[name]):
                try:
                    return float(row[name])
                except (ValueError, TypeError):
                    continue
        return 0.0

class YOLOTrainingManager:
    _instance = None
    _lock = threading.Lock()
    
    def __new__(cls):
        if cls._instance is None:
            with cls._lock:
                if cls._instance is None:
                    cls._instance = super(YOLOTrainingManager, cls).__new__(cls)
        return cls._instance
    
    def __init__(self):
        if hasattr(self, '_initialized'):
            return
        
        self.active_processes = {}
        self.csv_monitors = {}
        self.file_observers = {}
        self.training_threads = {}
        self.stop_events = {}
        self.csv_handlers = {}
        self.training_processes = {}  # åªè®°å½•è®­ç»ƒå­è¿›ç¨‹
        self._initialized = True
        self.time_stamp =str(time.strftime("%Y%m%d%H%M", time.localtime()))
        self.save_folder_id = ''
        # self.save_folder_id = f"train-folder-{self.time_stamp}-{str(uuid4())[20:]}" # è®­ç»ƒæ–‡ä»¶å¤¹å­˜å‚¨id
        
        
        logger.info("YOLOTrainingManager åˆå§‹åŒ–å®Œæˆ")
    
    def stop_training(self, session_id):
        """ç²¾ç¡®åœæ­¢è®­ç»ƒ - ä¿®å¤ç‰ˆ"""
        if session_id not in self.active_processes:
            return False, "è®­ç»ƒä¼šè¯ä¸å­˜åœ¨"
        
        try:
            logger.info(f"ğŸ›‘ å¼€å§‹åœæ­¢è®­ç»ƒ: {session_id}")
            
            if session_id in self.stop_events:
                self.stop_events[session_id].set()
                logger.info(f"âœ… è®¾ç½®åœæ­¢äº‹ä»¶: {session_id}")
            
            self._stop_csv_monitoring(session_id)
            success = self._force_stop_training_process(session_id)
            
            if session_id in self.training_threads:
                training_thread = self.training_threads[session_id]
                training_thread.join(timeout=5.0)
                
                if training_thread.is_alive():
                    logger.warning(f"âš ï¸ è®­ç»ƒçº¿ç¨‹ {session_id} ä»åœ¨è¿è¡Œ")
                else:
                    logger.info(f"âœ… è®­ç»ƒçº¿ç¨‹ {session_id} å·²åœæ­¢")
            
            if session_id in self.active_processes:
                self.active_processes[session_id]['status'] = 'stopped'
            
            if torch.cuda.is_available():
                torch.cuda.empty_cache()
            
            self._cleanup_session_resources(session_id)
            
            message = "è®­ç»ƒå·²ç²¾ç¡®åœæ­¢" if success else "è®­ç»ƒåœæ­¢ä¿¡å·å·²å‘é€"
            return True, message
            
        except Exception as e:
            logger.error(f"åœæ­¢è®­ç»ƒå‡ºé”™: {e}")
            return False, f"åœæ­¢å¤±è´¥: {str(e)}"
    
    def _force_stop_training_process(self, session_id):
        """ç²¾ç¡®åœæ­¢è®­ç»ƒè¿›ç¨‹ - ä¿®å¤ç‰ˆï¼Œé¿å…è¯¯æ€FlaskæœåŠ¡"""
        try:
            current_pid = os.getpid()
            flask_process = psutil.Process(current_pid)
            
            # æ–¹æ³•1: å¦‚æœæœ‰è®°å½•çš„è®­ç»ƒå­è¿›ç¨‹ï¼Œç›´æ¥ç»ˆæ­¢
            if session_id in self.training_processes:
                process = self.training_processes[session_id]
                if process and process.poll() is None:
                    logger.info(f"ğŸ¯ ç»ˆæ­¢å·²è®°å½•çš„è®­ç»ƒå­è¿›ç¨‹: PID {process.pid}")
                    try:
                        # ç»ˆæ­¢å­è¿›ç¨‹åŠå…¶æ‰€æœ‰å­è¿›ç¨‹
                        parent = psutil.Process(process.pid)
                        children = parent.children(recursive=True)
                        
                        # å…ˆç»ˆæ­¢å­è¿›ç¨‹
                        for child in children:
                            try:
                                child.terminate()
                                logger.info(f"ğŸ”¥ ç»ˆæ­¢å­è¿›ç¨‹: PID {child.pid}")
                            except psutil.NoSuchProcess:
                                pass
                        
                        # å†ç»ˆæ­¢ä¸»è¿›ç¨‹
                        parent.terminate()
                        
                        # ç­‰å¾…è¿›ç¨‹ç»“æŸ
                        try:
                            parent.wait(timeout=3)
                            logger.info(f"âœ… è®­ç»ƒè¿›ç¨‹å·²æ­£å¸¸ç»ˆæ­¢")
                            return True
                        except psutil.TimeoutExpired:
                            # å¼ºåˆ¶æ€æ­»
                            for child in children:
                                try:
                                    child.kill()
                                except psutil.NoSuchProcess:
                                    pass
                            parent.kill()
                            logger.info(f"âš¡ å¼ºåˆ¶æ€æ­»è®­ç»ƒè¿›ç¨‹")
                            return True
                            
                    except psutil.NoSuchProcess:
                        logger.info(f"â„¹ï¸ è®­ç»ƒè¿›ç¨‹å·²ä¸å­˜åœ¨")
                        return True
                    except Exception as e:
                        logger.error(f"ç»ˆæ­¢è®­ç»ƒè¿›ç¨‹æ—¶å‡ºé”™: {e}")
            
            # æ–¹æ³•2: æŸ¥æ‰¾ç‰¹å®šçš„è®­ç»ƒè¿›ç¨‹ï¼ˆæ›´ç²¾ç¡®çš„æ¡ä»¶ï¼‰
            terminated_count = 0
            
            # åªæŸ¥æ‰¾Flaskè¿›ç¨‹çš„ç›´æ¥å­è¿›ç¨‹
            try:
                flask_children = flask_process.children(recursive=True)
                
                for child in flask_children:
                    try:
                        cmdline = ' '.join(child.cmdline())
                        
                        # æ›´ç²¾ç¡®çš„åŒ¹é…æ¡ä»¶ï¼š
                        # 1. åŒ…å«yoloè®­ç»ƒç›¸å…³å…³é”®è¯
                        # 2. æ˜¯Pythonè¿›ç¨‹
                        # 3. ä¸æ˜¯Flaskä¸»è¿›ç¨‹
                        # 4. åŒ…å«trainç›¸å…³å‚æ•°
                        if (child.name() in ['python', 'python3'] and 
                            any(keyword in cmdline.lower() for keyword in ['yolo', 'ultralytics']) and
                            'train' in cmdline.lower() and
                            'app.py' not in cmdline and  # æ’é™¤Flaskä¸»è¿›ç¨‹
                            'flask' not in cmdline.lower() and  # æ’é™¤Flaskç›¸å…³è¿›ç¨‹
                            child.pid != current_pid):
                            
                            logger.info(f"ğŸ¯ æ‰¾åˆ°è®­ç»ƒå­è¿›ç¨‹: PID {child.pid}")
                            logger.info(f"ğŸ” è¿›ç¨‹å‘½ä»¤: {cmdline[:200]}")
                            
                            child.terminate()
                            terminated_count += 1
                            
                            try:
                                child.wait(timeout=3)
                            except psutil.TimeoutExpired:
                                try:
                                    child.kill()
                                    logger.info(f"âš¡ å¼ºåˆ¶æ€æ­»è¿›ç¨‹: PID {child.pid}")
                                except psutil.NoSuchProcess:
                                    pass
                                    
                    except (psutil.NoSuchProcess, psutil.AccessDenied):
                        continue
                        
            except Exception as e:
                logger.error(f"æŸ¥æ‰¾å­è¿›ç¨‹æ—¶å‡ºé”™: {e}")
            
            if terminated_count > 0:
                logger.info(f"âœ… ç²¾ç¡®ç»ˆæ­¢äº† {terminated_count} ä¸ªè®­ç»ƒè¿›ç¨‹")
                return True
            else:
                logger.info("â„¹ï¸ æœªæ‰¾åˆ°éœ€è¦ç»ˆæ­¢çš„è®­ç»ƒè¿›ç¨‹")
                return False
                
        except Exception as e:
            logger.error(f"ç²¾ç¡®åœæ­¢è¿›ç¨‹å¤±è´¥: {e}")
            return False
    
    def _run_training(self, session_id, params, stop_event):
        """è¿è¡Œè®­ç»ƒ - æ”¹è¿›å­è¿›ç¨‹ç®¡ç†"""
        try:
            logger.info(f"ğŸƒ å¼€å§‹è®­ç»ƒä¼šè¯: {session_id}")
            self.active_processes[session_id]['status'] = 'running'
            
            threading.Thread(
                target=self._enhanced_csv_monitoring,
                args=(session_id, params),
                daemon=True
            ).start()
            
            # ä½¿ç”¨å­è¿›ç¨‹è¿è¡Œè®­ç»ƒ
            actual_save_dir = self._run_training_subprocess(session_id, params, stop_event)
            
            if actual_save_dir:
                self.active_processes[session_id]['save_dir'] = actual_save_dir
                logger.info(f"ğŸ“ å®é™…ä¿å­˜ç›®å½•: {actual_save_dir}")
            
            if stop_event.is_set():
                self.active_processes[session_id]['status'] = 'stopped'
                logger.info(f"ğŸ›‘ è®­ç»ƒè¢«åœæ­¢: {session_id}")
            else:
                self.active_processes[session_id]['status'] = 'completed'
                logger.info(f"âœ… è®­ç»ƒå®Œæˆ: {session_id}")
                
                # if actual_save_dir:
                #     self._auto_zip_training_results(actual_save_dir)
            
        except InterruptedError:
            logger.info(f"ğŸ›‘ è®­ç»ƒè¢«ä¸­æ–­: {session_id}")
            if session_id in self.active_processes:
                self.active_processes[session_id]['status'] = 'stopped'
        except Exception as e:
            logger.error(f"âŒ è®­ç»ƒé”™è¯¯ {session_id}: {e}")
            if session_id in self.active_processes:
                self.active_processes[session_id]['status'] = 'error'
                self.active_processes[session_id]['error'] = str(e)
        finally:
            self._cleanup_session_resources(session_id)
    
    def _run_training_subprocess(self, session_id, params, stop_event):
        """åœ¨å­è¿›ç¨‹ä¸­è¿è¡Œè®­ç»ƒ - æ”¹è¿›ç‰ˆ"""
        try:
            # ç›´æ¥è°ƒç”¨è®­ç»ƒå‡½æ•°ï¼Œè€Œä¸æ˜¯åˆ›å»ºè„šæœ¬
            # è¿™æ ·å¯ä»¥æ›´å¥½åœ°æ§åˆ¶è¿›ç¨‹
            actual_save_dir = None
            
            def training_worker():
                nonlocal actual_save_dir
                try:
                    params['stop_event'] = stop_event
                    actual_save_dir = standard_train(
                        config_yaml_path=params['config_yaml_path'],
                        data_yaml=params['data_yaml'],
                        save_path= params['save_path'],
                        batch_size=params['batch_size'],
                        epochs = params['epochs'],
                        image_size=params['image_size'],
                        learning_rate=params['learning_rate'],
                        device=params['device'],
                        rtd_yolo=params['rtd_yolo'],
                        session_id=self.save_folder_id
                    )
                except Exception as e:
                    logger.error(f"è®­ç»ƒå­è¿›ç¨‹å‡ºé”™: {e}")
                    raise
            
            # åœ¨çº¿ç¨‹ä¸­è¿è¡Œè®­ç»ƒï¼ˆè€Œä¸æ˜¯å­è¿›ç¨‹ï¼‰
            # è¿™æ ·æ›´å®¹æ˜“æ§åˆ¶å’Œåœæ­¢
            training_thread = threading.Thread(target=training_worker, daemon=True)
            training_thread.start()
            
            # ç›‘æ§è®­ç»ƒçº¿ç¨‹
            while training_thread.is_alive():
                if stop_event.is_set():
                    logger.info(f"ğŸ›‘ æ£€æµ‹åˆ°åœæ­¢ä¿¡å·")
                    break
                training_thread.join(timeout=1)
            
            # ç­‰å¾…çº¿ç¨‹å®Œæˆ
            training_thread.join(timeout=5)
            
            return actual_save_dir
                
        except Exception as e:
            logger.error(f"è®­ç»ƒæ‰§è¡Œå¤±è´¥: {e}")
            raise
    
    def start_training(self, config):
        """å¯åŠ¨YOLOè®­ç»ƒ"""
        self.save_folder_id = f"train-folder-{self.time_stamp}-{str(uuid4())[20:]}" # è®­ç»ƒæ–‡ä»¶å¤¹å­˜å‚¨id
        session_id = str(self.save_folder_id)
        
        try:
            # if not self._validate_config(config):
            #     return None, False, "é…ç½®éªŒè¯å¤±è´¥", None
            
            training_params = self._prepare_training_params(config)
            
            # è®­ç»ƒæ–‡ä»¶å¤¹
            # record_msg = \
            #         {
            #             "file_id":str(self.save_folder_id),
            #             "file_folder_id":str(self.save_folder_id),
            #             "file_path":str(training_params['save_path']),
            #             "file_folder_path":str(training_params['save_path']),
            #             "file_real_name":str(training_params['name']), # å‰ç«¯ä¼ å…¥çš„é¡¹ç›®åç§°
            #             "file_type":'folder',
            #             'file_comment':'trained-weights',
            #             'dataset_id':str(config.get('trainData')),
            #             "is_detected":False,
            #             "file_create_time": self.time_stamp,
            #             "session_id":str(session_id),
            #         }
            # data_record(record_msg,fieldnames=table_5(),save_path=files_record_mapping()[str(FilesType.weights)][1])
            record_msg = WeightTable(
                    file_id = str(self.save_folder_id),
                    file_path = str(training_params['save_path']),
                    folder_path = str(training_params['save_path']),
                    file_name = str(training_params['name']),
                    dataset_id = str(config.get('trainData')),
                    session_id = str(session_id),
                    train_log = str(os.path.join(project_path, '_api/logs/train',f"{self.save_folder_id}.log"))
            )
            db.session.add(record_msg)
            db.session.commit()
            
            logger.info(f"ğŸš€ å¯åŠ¨è®­ç»ƒï¼Œå‚æ•°: {training_params}")

            stop_event = threading.Event()
            self.stop_events[session_id] = stop_event
            
            self.active_processes[session_id] = {
                'status': 'starting',
                'config': config,
                'params': training_params,
                'start_time': time.time(),
                'latest_data': None,
                'save_dir': None,
                'stop_event': stop_event,
                'csv_path': None
            }
            
            training_thread = threading.Thread(
                target=self._run_training,
                args=(session_id, training_params, stop_event),
                daemon=True
            )
            self.training_threads[session_id] = training_thread
            training_thread.start()
            
            save_dir = training_params['save_path']
            self.active_processes[session_id]['save_dir'] = save_dir
            
            return session_id, True, "è®­ç»ƒå¯åŠ¨æˆåŠŸ", save_dir,self.save_folder_id
            
        except Exception as e:
            # files_info("file_id", self.save_folder_id, delete_row=True,record_path=files_record_mapping()[str(FilesType.weights)][1])
            
            logger.error(f"âŒ å¯åŠ¨å¤±è´¥è¯¦æƒ…: {e}")
            if session_id in self.stop_events:
                del self.stop_events[session_id]
            if session_id in self.active_processes:
                del self.active_processes[session_id]
            return None, False, f"è®­ç»ƒå¯åŠ¨å¤±è´¥: {str(e)}", None, None
    
    def _stop_csv_monitoring(self, session_id):
        """åœæ­¢CSVç›‘æ§"""
        try:
            if session_id in self.file_observers:
                observer = self.file_observers[session_id]
                if observer.is_alive():
                    observer.stop()
                    observer.join(timeout=5.0)
                del self.file_observers[session_id]
                logger.info(f"âœ… åœæ­¢æ–‡ä»¶ç›‘æ§å™¨: {session_id}")
            
            if session_id in self.csv_handlers:
                del self.csv_handlers[session_id]
            
            if session_id in self.csv_monitors:
                del self.csv_monitors[session_id]
                
        except Exception as e:
            logger.error(f"åœæ­¢CSVç›‘æ§å¤±è´¥: {e}")
    
    def _cleanup_session_resources(self, session_id):
        """æ¸…ç†ä¼šè¯èµ„æº"""
        try:
            resources_to_clean = [
                (self.stop_events, "åœæ­¢äº‹ä»¶"),
                (self.training_threads, "è®­ç»ƒçº¿ç¨‹"),
                (self.training_processes, "è®­ç»ƒè¿›ç¨‹"),
                (self.csv_handlers, "CSVå¤„ç†å™¨"),
                (self.csv_monitors, "CSVç›‘æ§"),
                (self.file_observers, "æ–‡ä»¶è§‚å¯Ÿå™¨")
            ]
            
            for resource_dict, resource_name in resources_to_clean:
                if session_id in resource_dict:
                    del resource_dict[session_id]
                    logger.info(f"ğŸ§¹ æ¸…ç†{resource_name}: {session_id}")
            
            logger.info(f"âœ… ä¼šè¯èµ„æºæ¸…ç†å®Œæˆ: {session_id}")
            
        except Exception as e:
            logger.error(f"æ¸…ç†èµ„æºå¤±è´¥: {e}")
    
    def get_training_status(self, session_id=None):
        """è·å–è®­ç»ƒçŠ¶æ€"""
        if session_id:
            raw_data = self.active_processes.get(session_id)
            if not raw_data:
                return None
            
            return {
                'status': raw_data.get('status'),
                'start_time': raw_data.get('start_time'),
                'csv_path': raw_data.get('csv_path'),
                'save_dir': raw_data.get('save_dir'),
                'latest_data': raw_data.get('latest_data'),
                'error': raw_data.get('error'),
                'config': raw_data.get('config', {}),
                'params': {k: v for k, v in raw_data.get('params', {}).items() 
                          if k != 'stop_event' and not callable(v)}
            }
        else:
            result = {}
            for sid, raw_data in self.active_processes.items():
                result[sid] = {
                    'status': raw_data.get('status'),
                    'start_time': raw_data.get('start_time'),
                    'csv_path': raw_data.get('csv_path'),
                    'save_dir': raw_data.get('save_dir'),
                    'latest_data': raw_data.get('latest_data'),
                    'error': raw_data.get('error')
                }
            return result
    
    def get_latest_progress(self, session_id):
        """è·å–æœ€æ–°çš„è®­ç»ƒè¿›åº¦"""
        if session_id in self.active_processes:
            return self.active_processes[session_id].get('latest_data')
        return None
    
    def _validate_config(self, config):
        """éªŒè¯è®­ç»ƒé…ç½®"""
        try:
            required_fields = ['data_yaml', 'config_yaml_path']
            for field in required_fields:
                if not config.get(field):
                    logger.error(f"âŒ ç¼ºå°‘å¿…è¦å‚æ•°: {field}")
                    return False
            
            files_to_check = [
                ('data_yaml', config.get('data_yaml')),
                ('config_yaml_path', config.get('config_yaml_path'))
            ]
            
            for field_name, file_path in files_to_check:
                if file_path and not os.path.exists(file_path):
                    logger.error(f"âŒ æ–‡ä»¶ä¸å­˜åœ¨ {field_name}: {file_path}")
                    return False
            
            logger.info("âœ… é…ç½®éªŒè¯é€šè¿‡")
            return True
            
        except Exception as e:
            logger.error(f"âŒ é…ç½®éªŒè¯é”™è¯¯: {e}")
            return False
    
    def _prepare_training_params(self, config):
        """è®­ç»ƒå‚æ•°"""
        params = {
            'config_yaml_path': config.get('version', 'yolov8n.pt'),
            'data_yaml_id': config.get('trainData'),
            # 'weight_path': config.get('weight_path'),
            # 'pr_name': config.get('name', ''),
            # 'desc': config.get('desc', ''),
            'save_path': str(os.path.join(project_path, '_api/data', 'train',self.save_folder_id)),
            'batch_size': int(config.get('batch', 8)),
            'epochs': int(config.get('epoch', 100)),
            'image_size': int(config.get('size', 640)),
            'learning_rate': float(config.get('lr', 0.01)),
            'device': config.get('device', 'cpu'),
            # 'freeze': config.get('freeze',None),
            'rtd_yolo': config.get('type', 'yolo'),
            # 'data_yaml':str(files_info('file_id',config.get('trainData'),record_path=files_record_mapping()[str(FilesType.datasets)][1])['yaml_path']),
            'name':config.get('name', str(self.save_folder_id)),
            'data_yaml':str(DatasetTable.query.filter_by(file_id=config.get('trainData')).first().yaml_path),
        }
        
        # if str(params['device']).lower() == 'cpu':
        #     params['device'] = 'cpu'
        # else:
        #     try:
        #         params['device'] = int(params['device'])
        #     except:
        #         params['device'] = 0
        
        return params
    
    def _enhanced_csv_monitoring(self, session_id, params):
        """å¢å¼ºçš„CSVç›‘æ§"""
        logger.info(f"ğŸ” å¼€å§‹å¢å¼ºCSVç›‘æ§: {session_id}")
        
        save_path = params.get('save_path', './runs/detect')
        possible_base_dirs = [
            save_path,
            './runs/detect', 
            './runs', 
            './runs/train'
        ]
        
        max_wait_time = 600
        check_interval = 3
        
        for elapsed in range(0, max_wait_time, check_interval):
            if session_id not in self.active_processes:
                logger.info(f"ä¼šè¯ {session_id} å·²ç»“æŸï¼Œåœæ­¢CSVç›‘æ§")
                break
            
            found_csv = None
            
            for base_dir in possible_base_dirs:
                if not os.path.exists(base_dir):
                    continue
                
                try:
                    csv_files = glob.glob(os.path.join(base_dir, "**/results.csv"), recursive=True)
                    
                    if csv_files:
                        latest_csv = max(csv_files, key=lambda f: os.path.getmtime(f) if os.path.exists(f) else 0)
                        
                        if time.time() - os.path.getmtime(latest_csv) < 300:
                            found_csv = latest_csv
                            logger.info(f"âœ… æ‰¾åˆ°CSV: {found_csv}")
                            break
                            
                except Exception as e:
                    logger.warning(f"æœç´¢ {base_dir} æ—¶å‡ºé”™: {e}")
            
            if found_csv:
                self.active_processes[session_id]['csv_path'] = found_csv
                self._start_csv_monitoring(session_id, found_csv)
                return
            
            time.sleep(check_interval)
            if elapsed % 15 == 0:
                logger.info(f"â³ æœç´¢CSVæ–‡ä»¶ä¸­... ({elapsed}ç§’)")
        
        logger.warning(f"âŒ ç­‰å¾…{max_wait_time}ç§’åä»æœªæ‰¾åˆ°CSVæ–‡ä»¶")
    
    def _start_csv_monitoring(self, session_id, csv_path):
        """å¼€å§‹ç›‘æ§CSVæ–‡ä»¶"""
        try:
            self._stop_csv_monitoring(session_id)
            
            if not os.path.exists(csv_path):
                logger.warning(f"CSVæ–‡ä»¶ä¸å­˜åœ¨: {csv_path}")
                return False
            
            observer = Observer()
            handler = CSVMonitorHandler(session_id, csv_path, self)
            
            watch_dir = str(Path(csv_path).parent)
            
            try:
                observer.schedule(handler, watch_dir, recursive=False)
                observer.start()
                
                self.file_observers[session_id] = observer
                self.csv_handlers[session_id] = handler
                self.csv_monitors[session_id] = csv_path
                
                logger.info(f"âœ… å¼€å§‹ç›‘æ§CSVæ–‡ä»¶: {csv_path}")
                
                handler.read_and_update()
                self._start_polling_backup(session_id, handler)
                
                return True
                
            except OSError as e:
                if "inotify watch limit reached" in str(e):
                    logger.warning(f"âš ï¸ inotifyç›‘æ§é™åˆ¶è¾¾åˆ°ï¼Œåˆ‡æ¢åˆ°è½®è¯¢æ¨¡å¼: {session_id}")
                    observer.stop()
                    return self._start_polling_mode(session_id, csv_path)
                else:
                    raise
            
        except Exception as e:
            logger.error(f"âŒ å¯åŠ¨CSVç›‘æ§å¤±è´¥: {e}")
            return False
    
    def _start_polling_mode(self, session_id, csv_path):
        """è½®è¯¢æ¨¡å¼ç›‘æ§CSVæ–‡ä»¶"""
        try:
            handler = CSVMonitorHandler(session_id, csv_path, self)
            self.csv_handlers[session_id] = handler
            self.csv_monitors[session_id] = csv_path
            
            handler.read_and_update()
            self._start_polling_backup(session_id, handler)
            
            logger.info(f"âœ… å¯åŠ¨è½®è¯¢æ¨¡å¼ç›‘æ§: {csv_path}")
            return True
            
        except Exception as e:
            logger.error(f"âŒ å¯åŠ¨è½®è¯¢æ¨¡å¼å¤±è´¥: {e}")
            return False
    
    def _start_polling_backup(self, session_id, handler):
        """å¯åŠ¨è½®è¯¢å¤‡ä»½æœºåˆ¶"""
        def polling_worker():
            last_check_time = 0
            check_interval = 3
            
            while session_id in self.active_processes:
                try:
                    current_time = time.time()
                    if current_time - last_check_time >= check_interval:
                        status = self.active_processes[session_id].get('status')
                        if status == 'running':
                            handler.read_and_update()
                        elif status in ['completed', 'error', 'stopped']:
                            break
                        last_check_time = current_time
                    
                    time.sleep(1)
                    
                except Exception as e:
                    logger.error(f"è½®è¯¢ç›‘æ§é”™è¯¯: {e}")
                    time.sleep(5)
            
            logger.info(f"åœæ­¢è½®è¯¢ç›‘æ§: {session_id}")
        
        polling_thread = threading.Thread(target=polling_worker, daemon=True)
        polling_thread.start()
    
    def _auto_zip_training_results(self, save_dir):
        """è‡ªåŠ¨æ‰“åŒ…è®­ç»ƒç»“æœ"""
        try:
            if not os.path.exists(save_dir):
                logger.warning(f"ä¿å­˜ç›®å½•ä¸å­˜åœ¨ï¼Œæ— æ³•æ‰“åŒ…: {save_dir}")
                return
            
            training_folder = Path(save_dir)
            folder_name = training_folder.name
            parent_dir = training_folder.parent
            zip_path = parent_dir / f"{folder_name}.zip"
            
            logger.info(f"ğŸ—œï¸ å¼€å§‹æ‰“åŒ…è®­ç»ƒç»“æœ: {save_dir}")
            logger.info(f"ğŸ“¦ å‹ç¼©åŒ…è·¯å¾„: {zip_path}")
            
            with zipfile.ZipFile(zip_path, 'w', zipfile.ZIP_DEFLATED) as zipf:
                for root, dirs, files in os.walk(save_dir):
                    for file in files:
                        file_path = os.path.join(root, file)
                        arcname = os.path.relpath(file_path, parent_dir)
                        zipf.write(file_path, arcname)
                        logger.debug(f"ğŸ“„ æ·»åŠ æ–‡ä»¶: {arcname}")
            
            zip_size = os.path.getsize(zip_path)
            zip_size_mb = zip_size / (1024 * 1024)
            
            logger.info(f"âœ… æ‰“åŒ…å®Œæˆ!")
            logger.info(f"ğŸ“¦ å‹ç¼©åŒ…å¤§å°: {zip_size_mb:.2f} MB")
            
            print(f"=== è®­ç»ƒå®Œæˆï¼Œå‹ç¼©åŒ…è·¯å¾„ ===")
            print(f"å‹ç¼©åŒ…ä½ç½®: {zip_path}")
            print(f"å‹ç¼©åŒ…å¤§å°: {zip_size_mb:.2f} MB")
            print(f"åŸå§‹æ–‡ä»¶å¤¹: {save_dir}")
            print("================================")
            
            return str(zip_path)
            
        except Exception as e:
            logger.error(f"âŒ æ‰“åŒ…å¤±è´¥: {e}")
            return None